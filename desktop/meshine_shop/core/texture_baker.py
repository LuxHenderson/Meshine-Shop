"""
Texture baking module for Meshine Shop (Phase 2c).

Generates three PBR texture maps from the UV-mapped decimated mesh:

    albedo.png  — Diffuse/base color projected from the reconstruction source
                  (USDZ textures for Apple Object Capture, colored point cloud
                  for COLMAP). Resolution: 2048×2048 RGB PNG.

    normal.png  — Tangent-space normal map encoding surface orientation.
                  Generated from the mesh's own vertex normals. Enables
                  surface shading in game engines without baking from a
                  separate high-poly source. Resolution: 2048×2048 RGB PNG.

    ao.png      — Ambient occlusion map computed by hemisphere ray casting
                  from each mesh vertex using Open3D's C++ ray tracer.
                  Bright (white) = fully lit, dark (black) = occluded.
                  Resolution: 2048×2048 greyscale PNG.

All three maps are generated by UV-space triangle rasterization: for each
triangle in UV space, pixels are filled by scan-conversion and per-vertex
data (color, normal, AO value) is interpolated via barycentric coordinates.

Public API (called by engine bake_textures() implementations):
    bake_albedo_from_usdz(mesh, usdz_extract_dir, on_progress) → np.ndarray
    bake_albedo_from_pointcloud(mesh, pcd_path, on_progress) → np.ndarray
    vertex_colors_to_texture(uvs, faces, vertex_colors, image_size) → PIL Image
    bake_normal_map(mesh, image_size) → PIL Image
    bake_ao(mesh, num_rays, image_size, on_progress) → PIL Image
"""

import numpy as np
from pathlib import Path
from PIL import Image


# ---------------------------------------------------------------------------
# Configuration
# ---------------------------------------------------------------------------

# Default texture resolution. 2048×2048 is the standard for game asset textures
# — high enough for detailed results, small enough for fast baking.
DEFAULT_TEXTURE_SIZE = 2048

# Epsilon for barycentric coordinate inside-triangle test. A small negative
# value allows pixels exactly on triangle edges to be included, preventing
# hairline gaps between adjacent UV islands.
BARY_EPSILON = -1e-5

# Ray origin offset to prevent self-intersection during AO ray casting.
# Rays start slightly above the surface along the vertex normal.
AO_RAY_OFFSET = 1e-3


# ---------------------------------------------------------------------------
# UV-space triangle rasterization
# ---------------------------------------------------------------------------

def _rasterize_triangles(uvs, faces, vertex_data, image_size):
    """
    Rasterize UV-mapped mesh triangles into a texture image.

    For each triangle in UV space:
    1. Compute its pixel-space bounding box
    2. For each pixel in the bounding box, compute barycentric coordinates
    3. If inside the triangle, interpolate vertex_data at that pixel
    4. Write the interpolated value to the output image

    This is standard software UV rasterization — the same algorithm used
    inside Blender's bake system and Substance Painter.

    Args:
        uvs:         (N, 2) float32 UV coordinates in [0, 1]² space.
        faces:       (F, 3) int32 triangle face indices into uvs.
        vertex_data: (N, C) float32 per-vertex data to interpolate
                     (RGB colors, normal vectors, AO values, etc.).
        image_size:  int — output texture resolution (square).

    Returns:
        numpy array of shape (image_size, image_size, C) with float32 values.
        Unfilled pixels (outside all UV islands) remain as 0.
    """
    n_channels = vertex_data.shape[1]
    img = np.zeros((image_size, image_size, n_channels), dtype=np.float32)

    # Convert UV coordinates to pixel coordinates.
    # U (horizontal) maps directly to X. V (vertical) is flipped because UV
    # has origin at bottom-left but image arrays have origin at top-left.
    px = uvs[:, 0] * (image_size - 1)                   # U → pixel X
    py = (1.0 - uvs[:, 1]) * (image_size - 1)           # V → pixel Y (flipped)

    for face in faces:
        i0, i1, i2 = int(face[0]), int(face[1]), int(face[2])

        # Triangle vertex positions in pixel space.
        x0, y0 = px[i0], py[i0]
        x1, y1 = px[i1], py[i1]
        x2, y2 = px[i2], py[i2]

        # Per-vertex data to be interpolated across this triangle.
        d0 = vertex_data[i0]
        d1 = vertex_data[i1]
        d2 = vertex_data[i2]

        # Axis-aligned bounding box of this triangle in pixel space.
        # Clamp to image bounds to avoid out-of-range writes.
        xmin = max(0, int(np.floor(min(x0, x1, x2))))
        xmax = min(image_size - 1, int(np.ceil(max(x0, x1, x2))))
        ymin = max(0, int(np.floor(min(y0, y1, y2))))
        ymax = min(image_size - 1, int(np.ceil(max(y0, y1, y2))))

        if xmin > xmax or ymin > ymax:
            continue  # Triangle outside image bounds

        # Denominator of the barycentric coordinate formula.
        # Uses the signed area of the triangle (cross product magnitude).
        denom = (y1 - y2) * (x0 - x2) + (x2 - x1) * (y0 - y2)
        if abs(denom) < 1e-8:
            continue  # Degenerate / zero-area triangle

        # Generate pixel coordinates for the entire bounding box at once.
        # np.meshgrid creates 2D arrays of (x, y) for every pixel in the bbox.
        xs = np.arange(xmin, xmax + 1, dtype=np.float32)
        ys = np.arange(ymin, ymax + 1, dtype=np.float32)
        xx, yy = np.meshgrid(xs, ys)

        # Compute barycentric coordinates for all bbox pixels simultaneously.
        # w0, w1, w2 are the weights of each vertex's contribution at each pixel.
        w0 = ((y1 - y2) * (xx - x2) + (x2 - x1) * (yy - y2)) / denom
        w1 = ((y2 - y0) * (xx - x2) + (x0 - x2) * (yy - y2)) / denom
        w2 = 1.0 - w0 - w1

        # Pixels inside the triangle have all weights >= 0.
        # BARY_EPSILON allows edge pixels to be included (prevents UV seam gaps).
        inside = (w0 >= BARY_EPSILON) & (w1 >= BARY_EPSILON) & (w2 >= BARY_EPSILON)
        if not inside.any():
            continue

        # Get the local (row, col) indices of inside pixels.
        iy, ix = np.where(inside)

        # Interpolate vertex data at each inside pixel using barycentric weights.
        # Expand weights to (P, 1) so they broadcast correctly against (C,) data.
        w0_i = w0[iy, ix][:, np.newaxis]
        w1_i = w1[iy, ix][:, np.newaxis]
        w2_i = w2[iy, ix][:, np.newaxis]
        interpolated = w0_i * d0 + w1_i * d1 + w2_i * d2

        # Write to the full image at the correct pixel coordinates.
        # ymin + iy converts local bbox row indices to image row indices.
        img[ymin + iy, xmin + ix] = interpolated

    return img


# ---------------------------------------------------------------------------
# Nearest-neighbour color lookup
# ---------------------------------------------------------------------------

def _nearest_neighbor_colors(query_verts, source_verts, source_colors):
    """
    Assign a color to each query vertex by finding its nearest source point.

    Used to transfer color data from a point cloud (with RGB) to a mesh
    (with no color). Open3D's KDTreeFlann performs the nearest-neighbour
    search efficiently in C++.

    Args:
        query_verts:   (N, 3) float64 mesh vertex positions.
        source_verts:  (M, 3) float64 point cloud positions.
        source_colors: (M, 3) float32 point cloud RGB colors in [0, 1].

    Returns:
        (N, 3) float32 array of RGB colors in [0, 1] for each query vertex.
    """
    import open3d as o3d

    # Build a KD-tree from the source point cloud for fast nearest-neighbour queries.
    pcd = o3d.geometry.PointCloud()
    pcd.points = o3d.utility.Vector3dVector(source_verts.astype(np.float64))
    kd_tree = o3d.geometry.KDTreeFlann(pcd)

    result_colors = np.zeros((len(query_verts), 3), dtype=np.float32)

    for i, v in enumerate(query_verts):
        # search_knn_vector_3d returns (k, [indices], [sq_distances]).
        # k=1 finds the single nearest point.
        [_, idx, _] = kd_tree.search_knn_vector_3d(v.astype(np.float64), 1)
        result_colors[i] = source_colors[idx[0]]

    return result_colors


# ---------------------------------------------------------------------------
# Albedo baking — Apple Object Capture (USDZ) path
# ---------------------------------------------------------------------------

def bake_albedo_from_usdz(mesh, usdz_extract_dir, on_progress):
    """
    Extract the diffuse texture from a USDZ extraction directory and transfer
    its colors to the UV-mapped decimated mesh.

    Apple Object Capture stores its results as a USDZ archive (ZIP) containing:
    - A .usdc geometry file with the original full-resolution mesh and UV coords
    - PNG texture images (diffuse, normal, roughness, metallic, AO)

    The extraction directory (workspace/mesh/usdz_extracted/) is created during
    the mesh_reconstruct stage and persists through the pipeline.

    Transfer approach:
    1. Find the diffuse texture image in the extracted directory
    2. Load the original USD mesh vertices and face-varying UV coordinates
    3. For each vertex in our decimated mesh, find the nearest vertex in the
       original mesh using Open3D KDTreeFlann
    4. Sample the diffuse texture at that vertex's approximate UV position
    5. Return per-vertex colors for rasterization

    Args:
        mesh:             trimesh.Trimesh — the UV-mapped decimated mesh.
        usdz_extract_dir: Path to workspace/mesh/usdz_extracted/.
        on_progress:      Callback for status messages.

    Returns:
        (N, 3) float32 per-vertex RGB colors in [0, 1], or None if unavailable.
    """
    import open3d as o3d

    usdz_extract_dir = Path(usdz_extract_dir)

    # --- Find the diffuse texture image ---
    # Object Capture uses consistent naming conventions for PBR maps.
    # Apple stores textures in a subdirectory (e.g. usdz_extracted/0/) so we
    # use rglob (recursive) instead of glob to search the full tree.
    # "tex" is included because Apple names its diffuse map *_tex0.png.
    diffuse_names = ["diffuse", "albedo", "baseColor", "basecolor", "color", "Diffuse", "tex"]

    texture_path = None
    for candidate_name in diffuse_names:
        # rglob searches usdz_extracted/ and ALL subdirectories.
        matches = list(usdz_extract_dir.rglob(f"*{candidate_name}*"))
        image_matches = [p for p in matches if p.suffix.lower() in (".png", ".jpg", ".jpeg")]
        # Exclude images that are clearly not diffuse (AO, normal, roughness, etc.)
        # This prevents accidentally picking baked_mesh_*_ao0.png when searching "ao".
        non_diffuse = {"_ao", "_norm", "_rough", "_metal", "_disp", "_emissive"}
        image_matches = [
            p for p in image_matches
            if not any(tag in p.stem.lower() for tag in non_diffuse)
        ]
        if image_matches:
            texture_path = image_matches[0]
            break

    if texture_path is None:
        # No named match — pick the largest PNG (likely the diffuse map).
        # rglob searches subdirectories too.
        png_files = list(usdz_extract_dir.rglob("*.png")) + list(usdz_extract_dir.rglob("*.jpg"))
        # Exclude known non-diffuse maps from the fallback too.
        non_diffuse = {"_ao", "_norm", "_rough", "_metal", "_disp", "_emissive"}
        png_files = [
            p for p in png_files
            if not any(tag in p.stem.lower() for tag in non_diffuse)
        ]
        if png_files:
            texture_path = max(png_files, key=lambda p: p.stat().st_size)

    if texture_path is None:
        on_progress("No diffuse texture found in USDZ — using fallback gray")
        return None

    on_progress(f"Found diffuse texture: {texture_path.name}")

    # --- Load the original USD mesh vertices and UVs ---
    usdc_files = list(usdz_extract_dir.glob("*.usdc"))
    if not usdc_files:
        on_progress("No .usdc geometry file found — using fallback gray")
        return None

    try:
        from pxr import Usd, UsdGeom

        stage = Usd.Stage.Open(str(usdc_files[0]))

        # Find the mesh prim in the USD stage.
        mesh_prim = None
        for prim in stage.Traverse():
            if prim.IsA(UsdGeom.Mesh):
                mesh_prim = UsdGeom.Mesh(prim)
                break

        if mesh_prim is None:
            on_progress("No mesh prim in USD — using fallback gray")
            return None

        # Read original mesh vertices and face vertex indices.
        orig_points = np.array(mesh_prim.GetPointsAttr().Get(), dtype=np.float32)
        orig_face_indices = np.array(mesh_prim.GetFaceVertexIndicesAttr().Get())

        # Read the face-varying UV primvar ("st" or "primvars:st").
        # USD stores UVs as faceVarying — one UV per face-vertex, not per vertex.
        uv_primvar = None
        for attr_name in ["primvars:st", "primvars:st_0", "primvars:uv"]:
            pv = UsdGeom.PrimvarsAPI(mesh_prim).GetPrimvar(attr_name.replace("primvars:", ""))
            if pv and pv.IsDefined():
                uv_primvar = pv
                break

        if uv_primvar is None:
            on_progress("No UV primvar in USD — using fallback gray")
            return None

        # Get face-varying UV values and optionally their index array.
        uv_values = np.array(uv_primvar.Get(), dtype=np.float32)  # (K, 2)
        uv_indices = uv_primvar.GetIndices()  # May be empty

        # Build per-vertex UVs by averaging face-varying UVs for each vertex.
        # This is approximate — vertices at UV seams get the mean of both sides.
        per_vertex_uvs = np.zeros((len(orig_points), 2), dtype=np.float32)
        uv_count = np.zeros(len(orig_points), dtype=np.int32)

        if uv_indices is not None and len(uv_indices) > 0:
            uv_indices_arr = np.array(uv_indices)
            for vi, uv_i in zip(orig_face_indices, uv_indices_arr):
                per_vertex_uvs[vi] += uv_values[uv_i]
                uv_count[vi] += 1
        else:
            # Direct face-varying: one uv_value per face-vertex in order
            for face_vi, vi in enumerate(orig_face_indices):
                if face_vi < len(uv_values):
                    per_vertex_uvs[vi] += uv_values[face_vi]
                    uv_count[vi] += 1

        # Avoid division by zero for isolated vertices.
        mask = uv_count > 0
        per_vertex_uvs[mask] /= uv_count[mask, np.newaxis]

    except Exception as e:
        on_progress(f"USD UV extraction failed: {e} — using fallback gray")
        return None

    # --- Load the diffuse texture ---
    try:
        diffuse_img = Image.open(texture_path).convert("RGB")
        diffuse_np = np.array(diffuse_img, dtype=np.float32) / 255.0  # (H, W, 3) in [0, 1]
        tex_h, tex_w = diffuse_np.shape[:2]
    except Exception as e:
        on_progress(f"Failed to load diffuse texture: {e} — using fallback gray")
        return None

    on_progress("Transferring USDZ colors to decimated mesh vertices...")

    # --- KD-tree: find nearest original vertex for each decimated mesh vertex ---
    decimated_verts = np.array(mesh.vertices, dtype=np.float32)

    pcd = o3d.geometry.PointCloud()
    pcd.points = o3d.utility.Vector3dVector(orig_points.astype(np.float64))
    kd_tree = o3d.geometry.KDTreeFlann(pcd)

    vertex_colors = np.zeros((len(decimated_verts), 3), dtype=np.float32)

    for i, v in enumerate(decimated_verts):
        # Find the nearest vertex in the original USDZ mesh.
        [_, idx, _] = kd_tree.search_knn_vector_3d(v.astype(np.float64), 1)
        orig_idx = idx[0]

        # Look up the texture color at this original vertex's UV position.
        u, v_uv = per_vertex_uvs[orig_idx]

        # Convert normalized UV to texture pixel coordinates.
        # V is flipped because image Y-axis points down but UV V points up.
        tex_x = int(np.clip(u * (tex_w - 1), 0, tex_w - 1))
        tex_y = int(np.clip((1.0 - v_uv) * (tex_h - 1), 0, tex_h - 1))

        vertex_colors[i] = diffuse_np[tex_y, tex_x]

    on_progress(
        f"Color transfer complete: {len(decimated_verts):,} vertices "
        f"sampled from {texture_path.name}"
    )
    return vertex_colors


# ---------------------------------------------------------------------------
# Albedo baking — COLMAP colored point cloud path
# ---------------------------------------------------------------------------

def bake_albedo_from_pointcloud(mesh, pcd_path, on_progress):
    """
    Assign albedo colors to mesh vertices by finding the nearest colored
    point in a COLMAP-reconstructed point cloud.

    COLMAP stores RGB color at every reconstructed 3D point (from the
    source photo colors used during feature matching). Both the dense
    fused.ply (from patch-match stereo, if CUDA ran) and the sparse
    points3D.ply (always present) contain this color data.

    Args:
        mesh:        trimesh.Trimesh — the UV-mapped decimated mesh.
        pcd_path:    Path to the PLY point cloud file.
        on_progress: Callback for status messages.

    Returns:
        (N, 3) float32 per-vertex RGB colors in [0, 1], or None if unavailable.
    """
    import open3d as o3d

    if not Path(pcd_path).exists():
        on_progress(f"Point cloud not found: {pcd_path}")
        return None

    on_progress(f"Loading colored point cloud: {Path(pcd_path).name}...")

    try:
        pcd = o3d.io.read_point_cloud(str(pcd_path))
    except Exception as e:
        on_progress(f"Failed to load point cloud: {e}")
        return None

    if len(pcd.points) == 0:
        on_progress("Point cloud is empty")
        return None

    if not pcd.has_colors():
        on_progress("Point cloud has no color data")
        return None

    source_verts = np.asarray(pcd.points, dtype=np.float32)
    source_colors = np.asarray(pcd.colors, dtype=np.float32)  # Already [0, 1]

    on_progress(
        f"Point cloud loaded: {len(source_verts):,} points. "
        "Assigning colors to mesh vertices..."
    )

    query_verts = np.array(mesh.vertices, dtype=np.float32)
    vertex_colors = _nearest_neighbor_colors(query_verts, source_verts, source_colors)

    on_progress(f"Color assignment complete: {len(query_verts):,} vertices colored")
    return vertex_colors


# ---------------------------------------------------------------------------
# Vertex colors → texture image
# ---------------------------------------------------------------------------

def vertex_colors_to_texture(uvs, faces, vertex_colors, image_size=DEFAULT_TEXTURE_SIZE):
    """
    Rasterize per-vertex RGB colors into a 2D texture image.

    This is the final step of the albedo baking pipeline: takes per-vertex
    colors (assigned from USDZ or point cloud) and paints them into the
    UV texture space using barycentric interpolation.

    Args:
        uvs:           (N, 2) float32 UV coordinates in [0, 1]² space.
        faces:         (F, 3) int32 face indices.
        vertex_colors: (N, 3) float32 RGB colors in [0, 1].
        image_size:    int — output texture resolution (default 2048).

    Returns:
        PIL Image (RGB, 2048×2048).
    """
    # Rasterize the colored UV mesh into a float image.
    img_data = _rasterize_triangles(uvs, faces, vertex_colors, image_size)

    # Clamp to [0, 1] and convert to uint8.
    img_uint8 = (np.clip(img_data, 0.0, 1.0) * 255).astype(np.uint8)
    return Image.fromarray(img_uint8, "RGB")


# ---------------------------------------------------------------------------
# Normal map baking
# ---------------------------------------------------------------------------

def _compute_tangent(normal):
    """
    Compute a tangent vector perpendicular to the given normal.

    Used to build the TBN (Tangent-Bitangent-Normal) frame for each vertex.
    Handles the degenerate case where the normal is nearly parallel to the
    default up vector by falling back to a different reference axis.

    Args:
        normal: (3,) float32 unit normal vector.

    Returns:
        (3,) float32 unit tangent vector.
    """
    # Choose a reference vector that isn't nearly parallel to the normal.
    if abs(normal[2]) < 0.99:
        ref = np.array([0.0, 0.0, 1.0], dtype=np.float32)
    else:
        ref = np.array([1.0, 0.0, 0.0], dtype=np.float32)

    tangent = np.cross(ref, normal)
    norm = np.linalg.norm(tangent)
    if norm < 1e-8:
        # Fallback if cross product is still near-zero.
        tangent = np.array([1.0, 0.0, 0.0], dtype=np.float32)
    else:
        tangent /= norm
    return tangent


def bake_normal_map(mesh, image_size=DEFAULT_TEXTURE_SIZE):
    """
    Generate a tangent-space normal map from the mesh's own vertex normals.

    The normal map encodes each vertex's surface orientation relative to its
    local tangent frame. For a mesh with smooth normals, this captures surface
    curvature at the polygon level. Game engines use this for per-pixel shading.

    Encoding convention (OpenGL standard):
        R = (Nx + 1) / 2  → [0, 255], neutral = 128 (X=0)
        G = (Ny + 1) / 2  → [0, 255], neutral = 128 (Y=0)
        B = (Nz + 1) / 2  → [128, 255], neutral = 255 (Z=1, pointing out)

    For each vertex, the tangent-space normal is computed by:
    1. Computing tangent + bitangent vectors from UV gradients (accumulate per vertex)
    2. Gram-Schmidt orthogonalization against the vertex normal
    3. Building the TBN matrix [tangent | bitangent | normal]
    4. Transforming the world-space vertex normal to tangent space

    Args:
        mesh:       trimesh.Trimesh with UV coordinates at mesh.visual.uv.
        image_size: int — output texture resolution.

    Returns:
        PIL Image (RGB, image_size×image_size).
    """
    vertices = np.array(mesh.vertices, dtype=np.float32)
    faces = np.array(mesh.faces, dtype=np.int32)
    uvs = np.array(mesh.visual.uv, dtype=np.float32)
    vertex_normals = np.array(mesh.vertex_normals, dtype=np.float32)

    n = len(vertices)

    # --- Accumulate tangent and bitangent vectors per vertex from UV gradients ---
    # For each triangle, compute the tangent/bitangent from the relationship
    # between 3D edge vectors and UV edge vectors. Accumulate per vertex.
    tangents = np.zeros((n, 3), dtype=np.float32)
    bitangents = np.zeros((n, 3), dtype=np.float32)

    for face in faces:
        i0, i1, i2 = face[0], face[1], face[2]

        # 3D edge vectors of the triangle.
        dv1 = vertices[i1] - vertices[i0]
        dv2 = vertices[i2] - vertices[i0]

        # UV edge vectors of the same triangle.
        duv1 = uvs[i1] - uvs[i0]
        duv2 = uvs[i2] - uvs[i0]

        # Solve the system [dv1, dv2] = [[duv1], [duv2]] × [T, B]
        # to find tangent T and bitangent B directions.
        denom = duv1[0] * duv2[1] - duv1[1] * duv2[0]
        if abs(denom) < 1e-10:
            continue

        r = 1.0 / denom
        tangent = (duv2[1] * dv1 - duv1[1] * dv2) * r
        bitangent = (duv1[0] * dv2 - duv2[0] * dv1) * r

        # Accumulate for all three vertices of this face.
        for idx in [i0, i1, i2]:
            tangents[idx] += tangent
            bitangents[idx] += bitangent

    # --- Compute tangent-space normals per vertex ---
    # For each vertex, orthogonalize tangent against normal (Gram-Schmidt),
    # build TBN matrix, and express the world-space normal in tangent space.
    tangent_space_normals = np.zeros((n, 3), dtype=np.float32)

    for i in range(n):
        normal = vertex_normals[i]
        tangent = tangents[i]

        # Gram-Schmidt: orthogonalize tangent against normal.
        tangent = tangent - np.dot(tangent, normal) * normal
        t_len = np.linalg.norm(tangent)

        if t_len < 1e-8:
            # Degenerate tangent — compute a fallback.
            tangent = _compute_tangent(normal)
        else:
            tangent /= t_len

        # Bitangent from cross product — handedness is consistent with
        # the UV convention since we derived it from the UV edges.
        bitangent = np.cross(normal, tangent)

        # TBN matrix: columns are [T, B, N]. Transforms from tangent space
        # to world space. The transpose transforms world → tangent space.
        tbn_T = np.stack([tangent, bitangent, normal], axis=0)  # (3, 3), rows are T, B, N

        # Transform the world-space vertex normal to tangent space.
        # In tangent space, a flat surface has normal = (0, 0, 1).
        # Curved surfaces will have Nz close to 1 but Nx, Ny non-zero.
        ts_normal = tbn_T @ normal  # (3,)
        tangent_space_normals[i] = ts_normal

    # --- Encode tangent-space normals as RGB ---
    # Standard convention: (N + 1) / 2 maps [-1, 1] range to [0, 1].
    # The blue channel (Z component) will be > 0.5 for outward-facing normals.
    encoded = (tangent_space_normals + 1.0) / 2.0  # (N, 3) in [0, 1]

    # Rasterize into texture image.
    img_data = _rasterize_triangles(uvs, faces, encoded, image_size)

    # Fill unfilled pixels (UV island gaps) with the flat normal (0.5, 0.5, 1.0).
    # This prevents visible artifacts at UV island borders in game engines.
    unfilled_mask = (img_data[:, :, 0] == 0) & (img_data[:, :, 1] == 0) & (img_data[:, :, 2] == 0)
    img_data[unfilled_mask] = [0.5, 0.5, 1.0]

    img_uint8 = (np.clip(img_data, 0.0, 1.0) * 255).astype(np.uint8)
    return Image.fromarray(img_uint8, "RGB")


# ---------------------------------------------------------------------------
# AO baking via hemisphere ray casting
# ---------------------------------------------------------------------------

def _generate_hemisphere_samples(num_samples):
    """
    Generate uniformly distributed ray directions in a Z-up unit hemisphere.

    Uses cosine-weighted sampling: rays are more concentrated near the pole
    (normal direction) and sparser near the horizon. This gives better AO
    quality than uniform sampling for the same ray count because near-horizon
    rays contribute less to diffuse illumination (Lambert's cosine law).

    Args:
        num_samples: int — number of ray directions to generate.

    Returns:
        (num_samples, 3) float32 unit vectors in the Z-up hemisphere.
    """
    # Cosine-weighted hemisphere sampling via Malley's method.
    # Map uniform random samples (u1, u2) to hemisphere directions.
    rng = np.random.default_rng(seed=42)  # Fixed seed for reproducibility
    u1 = rng.uniform(0, 1, num_samples)
    u2 = rng.uniform(0, 1, num_samples)

    # Polar coordinates in the hemisphere.
    theta = np.arccos(np.sqrt(u1))   # Polar angle: cos-weighted distribution
    phi = 2.0 * np.pi * u2           # Azimuthal angle: uniform

    # Spherical to Cartesian conversion (Z is "up" / normal direction).
    x = np.sin(theta) * np.cos(phi)
    y = np.sin(theta) * np.sin(phi)
    z = np.cos(theta)

    return np.stack([x, y, z], axis=1).astype(np.float32)


def bake_ao(mesh, num_rays=64, image_size=DEFAULT_TEXTURE_SIZE, on_progress=None):
    """
    Compute per-vertex ambient occlusion by hemisphere ray casting.

    For each vertex, casts num_rays rays distributed over a hemisphere oriented
    along the vertex normal. AO = 1 - (fraction of rays that hit the mesh).
    Unoccluded vertices (open areas) get bright AO values; occluded vertices
    (corners, crevices) get dark values.

    Ray casting is performed by Open3D's RaycastingScene which uses a C++
    BVH (Bounding Volume Hierarchy) for fast intersection queries. All rays
    are submitted in a single batch for maximum performance.

    Args:
        mesh:        trimesh.Trimesh with UV coordinates at mesh.visual.uv.
        num_rays:    int — rays per vertex (more = less noise, slower).
                     64 is a good balance between quality and speed.
        image_size:  int — output texture resolution.
        on_progress: Callback for status messages.

    Returns:
        PIL Image (greyscale 'L', image_size×image_size).
        White (255) = fully lit. Dark (0) = fully occluded.
    """
    import open3d as o3d

    vertices = np.array(mesh.vertices, dtype=np.float32)
    faces = np.array(mesh.faces, dtype=np.int32)
    uvs = np.array(mesh.visual.uv, dtype=np.float32)
    vertex_normals = np.array(mesh.vertex_normals, dtype=np.float32)

    n = len(vertices)

    if on_progress:
        on_progress(
            f"Building AO raycasting scene ({n:,} vertices, "
            f"{num_rays} rays each)..."
        )

    # --- Build Open3D raycasting scene from the mesh ---
    mesh_o3d = o3d.geometry.TriangleMesh()
    mesh_o3d.vertices = o3d.utility.Vector3dVector(vertices.astype(np.float64))
    mesh_o3d.triangles = o3d.utility.Vector3iVector(faces)

    # Convert to tensor mesh for the raycasting API (tensor geometry is faster
    # than legacy geometry for raycasting because it uses SIMD instructions).
    mesh_tensor = o3d.t.geometry.TriangleMesh.from_legacy(mesh_o3d)
    scene = o3d.t.geometry.RaycastingScene()
    scene.add_triangles(mesh_tensor)

    # --- Precompute hemisphere ray directions ---
    # hemisphere_local: (num_rays, 3) directions in Z-up local space.
    # We'll rotate these to each vertex's normal direction below.
    hemisphere_local = _generate_hemisphere_samples(num_rays)

    if on_progress:
        on_progress(f"Casting {n * num_rays:,} AO rays...")

    # --- Build all rays in a single batch (n*num_rays rays) ---
    # This allows Open3D to cast every ray in one C++ call rather than N calls.
    all_origins = np.zeros((n * num_rays, 3), dtype=np.float32)
    all_dirs = np.zeros((n * num_rays, 3), dtype=np.float32)

    for i in range(n):
        normal = vertex_normals[i]

        # Build an orthonormal frame (T, B, N) aligned to this vertex's normal.
        tangent = _compute_tangent(normal)
        bitangent = np.cross(normal, tangent)

        # Rotation matrix: R transforms from local Z-up to world normal-up.
        # Columns are the local frame axes expressed in world space.
        R = np.stack([tangent, bitangent, normal], axis=1)  # (3, 3)

        # Rotate the hemisphere samples to align with this vertex's normal.
        dirs = hemisphere_local @ R.T  # (num_rays, 3)

        # Offset the ray origin slightly above the surface to avoid
        # self-intersection (hitting the face the ray originates from).
        origin = vertices[i] + normal * AO_RAY_OFFSET

        # Write into the batch arrays.
        start = i * num_rays
        end = start + num_rays
        all_origins[start:end] = origin
        all_dirs[start:end] = dirs

    # --- Cast all rays in a single Open3D call ---
    # rays format: (N, 6) where each row is [ox, oy, oz, dx, dy, dz]
    rays = np.concatenate([all_origins, all_dirs], axis=1)  # (n*num_rays, 6)
    rays_tensor = o3d.core.Tensor(rays, dtype=o3d.core.Dtype.Float32)

    # count_intersections: returns how many times each ray intersects the mesh.
    # We only care if it hit at all (any hit = occluded), not how many times.
    hit_counts = scene.count_intersections(rays_tensor).numpy()  # (n*num_rays,)

    # --- Compute per-vertex AO values ---
    # Reshape to (n, num_rays): each row is the hit results for one vertex.
    hit_matrix = (hit_counts > 0).reshape(n, num_rays)

    # AO value = fraction of unoccluded rays.
    # 1.0 = all rays unblocked (open area, fully lit).
    # 0.0 = all rays blocked (deep crevice, fully occluded).
    ao_values = 1.0 - hit_matrix.mean(axis=1).astype(np.float32)  # (n,)

    if on_progress:
        avg_ao = ao_values.mean()
        on_progress(
            f"AO complete: avg {avg_ao:.2f} (1=lit, 0=occluded). "
            "Rasterizing to texture..."
        )

    # --- Rasterize per-vertex AO values into texture image ---
    ao_2d = ao_values.reshape(-1, 1)  # (n, 1) for rasterizer
    img_data = _rasterize_triangles(uvs, faces, ao_2d, image_size)

    # Fill unfilled pixels (UV island gaps) with fully lit (1.0).
    # This prevents dark borders around UV islands in game engine rendering.
    unfilled_mask = img_data[:, :, 0] == 0
    img_data[unfilled_mask, 0] = 1.0

    # Convert to greyscale PIL Image.
    img_uint8 = (np.clip(img_data[:, :, 0], 0.0, 1.0) * 255).astype(np.uint8)
    return Image.fromarray(img_uint8, "L")
